{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding Social Intent from Neural Oscillations\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "**COGS 118C — Signal Processing Course Project**\n",
    "\n",
    "**Research Question:** Can we classify whether an animal is interacting socially vs. exploring alone based on the spectral features of its neural calcium signals?\n",
    "\n",
    "---\n",
    "\n",
    "### Goals of this notebook\n",
    "\n",
    "1. **Acquire & load** data from the EDGE course notebooks (calcium traces, behavioral annotations)\n",
    "2. **Inspect** data shapes, types, sampling rates, and quality\n",
    "3. **Visualize** raw calcium signals and behavioral epoch structure\n",
    "4. **Preliminary spectral analysis** — PSD, spectrograms, wavelets on sample signals\n",
    "5. **Compare** spectral features between social vs. solo epochs\n",
    "6. **Identify** data quality issues, confounds, and scope for the full project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install numpy pandas matplotlib seaborn scipy h5py gdown pywt scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Signal processing\n",
    "from scipy import signal\n",
    "from scipy.signal import welch, butter, filtfilt, stft\n",
    "import pywt\n",
    "\n",
    "# Styling\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=1.1)\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 5)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Acquisition\n",
    "\n",
    "Our data comes from the **EDGE** (Education in Data and Guided Exploration) course notebooks. We download the relevant Colab notebooks and extract the data they reference.\n",
    "\n",
    "### Source notebooks\n",
    "\n",
    "| Notebook | Content | Priority |\n",
    "|----------|---------|----------|\n",
    "| Finding social behaviors | Behavioral annotations (social vs. non-social) | High |\n",
    "| Calcium demo | Raw calcium fluorescence traces | High |\n",
    "| Demixed calcium | Clean, source-separated signals | High |\n",
    "| Neural signals of social isolation | Calcium + social condition labels | High |\n",
    "| Analyzing social isolation | Behavioral data for isolation study | Medium |\n",
    "| social_bouts.00 | Compiled social bout timing data | Medium |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "NB_DIR = DATA_DIR / \"source_notebooks\"\n",
    "NB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Google Drive file IDs extracted from the Colab URLs in data.md\n",
    "NOTEBOOKS = {\n",
    "    \"finding_social_behaviors\": \"13HGdsrS4lYxZcfpr4snE-fKbKhgcSZbf\",\n",
    "    \"calcium_demo\": \"1bhdkHCeHoOg2z0FTgnPlPfIasmxYhcqr\",\n",
    "    \"motion_correction\": \"1hL9mE9kZ2nr_RX0W6N7-dEDReifEeowR\",\n",
    "    \"demixed_calcium\": \"1CEZ13yr_5usvYLXFmTtTaepH7bVPeCU3\",\n",
    "    \"analyzing_social_isolation\": \"1nqRrS3MS1ASBeJjJCLOUbxqvP5VSOW1j\",\n",
    "    \"neural_signals_social_isolation\": \"1OHi6j34edcKM-X2gYmyHYytb9zCTPEIn\",\n",
    "}\n",
    "\n",
    "# Also download the social_bouts data file\n",
    "SOCIAL_BOUTS_ID = \"1Lz5hya0W_sXpcIptcrnQzCOWoestkOmm\"\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR.resolve()}\")\n",
    "print(f\"Notebook directory: {NB_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download notebooks from Google Drive\n",
    "for name, file_id in NOTEBOOKS.items():\n",
    "    output_path = NB_DIR / f\"{name}.ipynb\"\n",
    "    if output_path.exists():\n",
    "        print(f\"  [skip] {name} already downloaded\")\n",
    "        continue\n",
    "    try:\n",
    "        url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "        gdown.download(url, str(output_path), quiet=True)\n",
    "        print(f\"  [ok]   {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [FAIL] {name}: {e}\")\n",
    "\n",
    "# Download social_bouts data\n",
    "bouts_path = DATA_DIR / \"social_bouts.ipynb\"\n",
    "if not bouts_path.exists():\n",
    "    try:\n",
    "        gdown.download(\n",
    "            f\"https://drive.google.com/uc?id={SOCIAL_BOUTS_ID}\",\n",
    "            str(bouts_path), quiet=True\n",
    "        )\n",
    "        print(f\"  [ok]   social_bouts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [FAIL] social_bouts: {e}\")\n",
    "\n",
    "print(\"\\nDownloaded files:\")\n",
    "for f in sorted(DATA_DIR.rglob(\"*\")):\n",
    "    if f.is_file():\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.relative_to(DATA_DIR)}  ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extract data URLs from downloaded notebooks\n",
    "\n",
    "The EDGE notebooks typically load data from Google Drive or hosted URLs. Let's parse the notebooks to find those data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_data_urls_from_notebook(nb_path):\n",
    "    \"\"\"Parse a .ipynb file and extract data-loading URLs and gdown calls.\"\"\"\n",
    "    urls = []\n",
    "    try:\n",
    "        with open(nb_path, \"r\") as f:\n",
    "            nb = json.load(f)\n",
    "        for cell in nb.get(\"cells\", []):\n",
    "            if cell[\"cell_type\"] != \"code\":\n",
    "                continue\n",
    "            source = \"\".join(cell[\"source\"])\n",
    "            # Match URLs (http/https, Google Drive, raw GitHub, etc.)\n",
    "            found = re.findall(r'[\"\\']?(https?://[^\\s\"\\'\\)]+)[\"\\']?', source)\n",
    "            urls.extend(found)\n",
    "            # Match gdown IDs\n",
    "            gdown_ids = re.findall(r'gdown\\.download\\([^)]*[\"\\']([a-zA-Z0-9_-]{20,})[\"\\']', source)\n",
    "            for gid in gdown_ids:\n",
    "                urls.append(f\"https://drive.google.com/uc?id={gid}\")\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"  Could not parse {nb_path.name}: {e}\")\n",
    "    return urls\n",
    "\n",
    "print(\"Data URLs found in source notebooks:\\n\")\n",
    "all_data_urls = {}\n",
    "for nb_file in sorted(NB_DIR.glob(\"*.ipynb\")):\n",
    "    urls = extract_data_urls_from_notebook(nb_file)\n",
    "    if urls:\n",
    "        all_data_urls[nb_file.stem] = urls\n",
    "        print(f\"  {nb_file.stem}:\")\n",
    "        for u in urls:\n",
    "            print(f\"    {u}\")\n",
    "    else:\n",
    "        print(f\"  {nb_file.stem}: (no URLs found)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Download extracted data files\n",
    "\n",
    "We attempt to download the actual data files referenced by the notebooks. These are typically `.h5`, `.npy`, `.csv`, or `.mat` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "RAW_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Filter for actual data file URLs (not pip install, not colab, not docs)\n",
    "SKIP_PATTERNS = [\"pip\", \"colab\", \"github.com/\", \"googleapis.com/auth\", \".git\", \"readme\"]\n",
    "DATA_EXTENSIONS = [\".h5\", \".hdf5\", \".npy\", \".npz\", \".csv\", \".mat\", \".pkl\", \".parquet\", \".zip\", \".tar\"]\n",
    "\n",
    "data_urls_to_download = []\n",
    "for nb_name, urls in all_data_urls.items():\n",
    "    for url in urls:\n",
    "        url_lower = url.lower()\n",
    "        if any(skip in url_lower for skip in SKIP_PATTERNS):\n",
    "            continue\n",
    "        # Keep Drive download links and files with data extensions\n",
    "        if \"drive.google.com\" in url_lower or any(url_lower.endswith(ext) for ext in DATA_EXTENSIONS):\n",
    "            data_urls_to_download.append((nb_name, url))\n",
    "        # Also keep raw content URLs (e.g. raw.githubusercontent.com)\n",
    "        elif \"raw.\" in url_lower or \"download\" in url_lower:\n",
    "            data_urls_to_download.append((nb_name, url))\n",
    "\n",
    "print(f\"Found {len(data_urls_to_download)} potential data URLs to download:\\n\")\n",
    "for nb_name, url in data_urls_to_download:\n",
    "    print(f\"  [{nb_name}] {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data files\n",
    "downloaded_files = []\n",
    "\n",
    "for nb_name, url in data_urls_to_download:\n",
    "    # Determine filename from URL or use a generated name\n",
    "    if \"drive.google.com\" in url:\n",
    "        # Use gdown for Google Drive\n",
    "        file_id = re.search(r'id=([a-zA-Z0-9_-]+)', url)\n",
    "        if file_id:\n",
    "            out_path = RAW_DIR / f\"{nb_name}_drive_{file_id.group(1)[:8]}\"\n",
    "            try:\n",
    "                result = gdown.download(url, str(out_path), quiet=True)\n",
    "                if result:\n",
    "                    downloaded_files.append(out_path)\n",
    "                    print(f\"  [ok]   {out_path.name} ({out_path.stat().st_size / 1024:.1f} KB)\")\n",
    "            except Exception as e:\n",
    "                print(f\"  [FAIL] {url}: {e}\")\n",
    "    else:\n",
    "        # Direct URL download\n",
    "        fname = url.split(\"/\")[-1].split(\"?\")[0]\n",
    "        if not fname or len(fname) > 100:\n",
    "            fname = f\"{nb_name}_data\"\n",
    "        out_path = RAW_DIR / fname\n",
    "        if out_path.exists():\n",
    "            print(f\"  [skip] {fname} already exists\")\n",
    "            downloaded_files.append(out_path)\n",
    "            continue\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, str(out_path))\n",
    "            downloaded_files.append(out_path)\n",
    "            print(f\"  [ok]   {fname} ({out_path.stat().st_size / 1024:.1f} KB)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [FAIL] {fname}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal files downloaded: {len(downloaded_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading & Inspection\n",
    "\n",
    "Let's inspect what we actually have. We check file types, shapes, and structure for every downloaded data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_file(filepath):\n",
    "    \"\"\"Inspect a data file and report its contents.\"\"\"\n",
    "    fp = Path(filepath)\n",
    "    size = fp.stat().st_size\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"File: {fp.name}  ({size / 1024:.1f} KB)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Try as HDF5\n",
    "    try:\n",
    "        with h5py.File(fp, \"r\") as f:\n",
    "            print(\"Format: HDF5\")\n",
    "            def print_hdf5(name, obj):\n",
    "                if isinstance(obj, h5py.Dataset):\n",
    "                    print(f\"  Dataset: {name}  shape={obj.shape}  dtype={obj.dtype}\")\n",
    "                elif isinstance(obj, h5py.Group):\n",
    "                    print(f\"  Group:   {name}/\")\n",
    "            f.visititems(print_hdf5)\n",
    "        return \"h5\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Try as numpy\n",
    "    try:\n",
    "        data = np.load(fp, allow_pickle=True)\n",
    "        if isinstance(data, np.lib.npyio.NpzFile):\n",
    "            print(\"Format: NPZ (compressed numpy)\")\n",
    "            for key in data.files:\n",
    "                print(f\"  Array: {key}  shape={data[key].shape}  dtype={data[key].dtype}\")\n",
    "        else:\n",
    "            print(f\"Format: NPY  shape={data.shape}  dtype={data.dtype}\")\n",
    "        return \"npy\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Try as CSV / pandas\n",
    "    try:\n",
    "        df = pd.read_csv(fp, nrows=5)\n",
    "        print(f\"Format: CSV  shape={df.shape}  columns={list(df.columns)}\")\n",
    "        print(df.head(3).to_string())\n",
    "        return \"csv\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Try as JSON / notebook\n",
    "    try:\n",
    "        with open(fp, \"r\") as f:\n",
    "            content = json.load(f)\n",
    "        if \"cells\" in content:\n",
    "            n_code = sum(1 for c in content[\"cells\"] if c[\"cell_type\"] == \"code\")\n",
    "            n_md = sum(1 for c in content[\"cells\"] if c[\"cell_type\"] == \"markdown\")\n",
    "            print(f\"Format: Jupyter notebook  ({n_code} code cells, {n_md} markdown cells)\")\n",
    "            return \"ipynb\"\n",
    "        else:\n",
    "            print(f\"Format: JSON  (top-level keys: {list(content.keys())[:10]})\")\n",
    "            return \"json\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Fallback — show first bytes\n",
    "    try:\n",
    "        with open(fp, \"rb\") as f:\n",
    "            header = f.read(200)\n",
    "        print(f\"Format: Unknown  (first bytes: {header[:50]})\")\n",
    "    except:\n",
    "        print(\"Format: Could not read\")\n",
    "    return \"unknown\"\n",
    "\n",
    "# Inspect all downloaded data files\n",
    "file_types = {}\n",
    "for fp in sorted(RAW_DIR.iterdir()):\n",
    "    if fp.is_file():\n",
    "        ft = inspect_file(fp)\n",
    "        file_types[fp.name] = ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load the primary data\n",
    "\n",
    "Based on the inspection above, load the key datasets into memory. \n",
    "\n",
    "> **Note:** Adjust the cell below based on what file formats were actually downloaded. The code handles the most common formats from EDGE notebooks: HDF5, numpy arrays, and CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ADAPTIVE DATA LOADER\n",
    "# Loads data based on whatever format was downloaded.\n",
    "# Edit the paths below if your data lives somewhere else.\n",
    "# ============================================================\n",
    "\n",
    "calcium_data = None      # Will hold calcium traces (neurons x time) or (time x neurons)\n",
    "behavior_labels = None   # Will hold behavioral state labels (time,) or epoch-level\n",
    "sampling_rate = None     # Hz — critical for spectral analysis\n",
    "\n",
    "# --- Attempt to load from HDF5 files ---\n",
    "h5_files = list(RAW_DIR.glob(\"*.h5\")) + list(RAW_DIR.glob(\"*.hdf5\"))\n",
    "for h5f in h5_files:\n",
    "    print(f\"\\nLoading HDF5: {h5f.name}\")\n",
    "    with h5py.File(h5f, \"r\") as f:\n",
    "        # Print all datasets to find calcium traces\n",
    "        def find_arrays(name, obj):\n",
    "            if isinstance(obj, h5py.Dataset) and len(obj.shape) >= 1:\n",
    "                print(f\"  {name}: shape={obj.shape}, dtype={obj.dtype}\")\n",
    "        f.visititems(find_arrays)\n",
    "\n",
    "# --- Attempt to load from numpy files ---\n",
    "npy_files = list(RAW_DIR.glob(\"*.npy\")) + list(RAW_DIR.glob(\"*.npz\"))\n",
    "for npf in npy_files:\n",
    "    print(f\"\\nLoading numpy: {npf.name}\")\n",
    "    data = np.load(npf, allow_pickle=True)\n",
    "    if isinstance(data, np.lib.npyio.NpzFile):\n",
    "        for key in data.files:\n",
    "            arr = data[key]\n",
    "            print(f\"  {key}: shape={arr.shape}, dtype={arr.dtype}\")\n",
    "            # Heuristic: largest 2D array is probably calcium data\n",
    "            if arr.ndim == 2 and (calcium_data is None or arr.size > calcium_data.size):\n",
    "                calcium_data = arr\n",
    "                print(f\"    -> Loaded as calcium_data\")\n",
    "    else:\n",
    "        print(f\"  shape={data.shape}, dtype={data.dtype}\")\n",
    "        if data.ndim == 2 and (calcium_data is None or data.size > calcium_data.size):\n",
    "            calcium_data = data\n",
    "            print(f\"  -> Loaded as calcium_data\")\n",
    "\n",
    "# --- Attempt to load from CSV files ---\n",
    "csv_files = list(RAW_DIR.glob(\"*.csv\"))\n",
    "for csvf in csv_files:\n",
    "    print(f\"\\nLoading CSV: {csvf.name}\")\n",
    "    df = pd.read_csv(csvf)\n",
    "    print(f\"  shape={df.shape}, columns={list(df.columns)[:10]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if calcium_data is not None:\n",
    "    print(f\"calcium_data loaded: shape={calcium_data.shape}\")\n",
    "else:\n",
    "    print(\"WARNING: No calcium data loaded yet — see Section 3.2 for manual loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Manual data loading (if automatic extraction failed)\n",
    "\n",
    "If the automatic download didn't find the data files, you can:\n",
    "\n",
    "1. **Open the source Colab notebooks** listed in `data.md`\n",
    "2. **Run the data-loading cells** in each notebook\n",
    "3. **Download the data** to `../data/raw/` using Colab's file browser\n",
    "4. **Update the paths below** and re-run\n",
    "\n",
    "Alternatively, paste the data loading code from the Colab notebooks directly below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MANUAL DATA LOADING\n",
    "# Uncomment and edit the appropriate section for your data.\n",
    "# ============================================================\n",
    "\n",
    "# --- Option A: Load from HDF5 ---\n",
    "# with h5py.File(\"../data/raw/YOUR_FILE.h5\", \"r\") as f:\n",
    "#     print(list(f.keys()))  # See what's inside\n",
    "#     calcium_data = f[\"YOUR_DATASET_KEY\"][:]  # Load into memory\n",
    "#     # behavior_labels = f[\"YOUR_LABELS_KEY\"][:]\n",
    "#     # sampling_rate = f.attrs.get(\"sampling_rate\", 20.0)  # Check attrs\n",
    "\n",
    "# --- Option B: Load from numpy ---\n",
    "# calcium_data = np.load(\"../data/raw/YOUR_FILE.npy\")\n",
    "# behavior_labels = np.load(\"../data/raw/YOUR_LABELS.npy\")\n",
    "\n",
    "# --- Option C: Load from CSV ---\n",
    "# df = pd.read_csv(\"../data/raw/YOUR_FILE.csv\")\n",
    "# calcium_data = df.iloc[:, 1:].values  # Assumes first col is time\n",
    "# time_axis = df.iloc[:, 0].values\n",
    "\n",
    "# --- Set sampling rate (CRITICAL — verify from your data source) ---\n",
    "if sampling_rate is None:\n",
    "    sampling_rate = 20.0  # Hz — typical for calcium imaging; CHANGE if different\n",
    "    print(f\"Using default sampling_rate = {sampling_rate} Hz\")\n",
    "    print(\"WARNING: Verify this matches your actual data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Calcium Signal Visualization\n",
    "\n",
    "Visualize the raw calcium traces to check for:\n",
    "- **Signal quality** — are there clear transients?\n",
    "- **Photobleaching** — exponential decay in baseline?\n",
    "- **Motion artifacts** — sudden jumps correlated across all neurons?\n",
    "- **Dynamic range** — what's the ΔF/F amplitude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calcium_data is None:\n",
    "    print(\"No calcium data loaded — generating synthetic data for demonstration.\")\n",
    "    print(\"Replace this with real data once available.\\n\")\n",
    "\n",
    "    # ----- Synthetic calcium-like data for method demonstration -----\n",
    "    sampling_rate = 20.0  # Hz\n",
    "    n_neurons = 30\n",
    "    duration = 600  # seconds (10 minutes)\n",
    "    n_timepoints = int(duration * sampling_rate)\n",
    "    t = np.arange(n_timepoints) / sampling_rate\n",
    "\n",
    "    calcium_data = np.zeros((n_neurons, n_timepoints))\n",
    "\n",
    "    # Create behavioral epochs: alternating social (1) and solo (0)\n",
    "    behavior_labels = np.zeros(n_timepoints, dtype=int)\n",
    "    epoch_starts = np.arange(0, n_timepoints, int(30 * sampling_rate))  # 30-sec epochs\n",
    "    for i, start in enumerate(epoch_starts):\n",
    "        end = min(start + int(30 * sampling_rate), n_timepoints)\n",
    "        if i % 2 == 1:  # Odd epochs are \"social\"\n",
    "            behavior_labels[start:end] = 1\n",
    "\n",
    "    for n in range(n_neurons):\n",
    "        # Baseline 1/f noise (characteristic of calcium signals)\n",
    "        freqs = np.fft.rfftfreq(n_timepoints, d=1/sampling_rate)\n",
    "        freqs[0] = 1  # Avoid division by zero\n",
    "        noise_spectrum = 1 / (freqs ** 0.8) * np.exp(1j * 2 * np.pi * np.random.rand(len(freqs)))\n",
    "        baseline = np.fft.irfft(noise_spectrum, n=n_timepoints)\n",
    "        baseline = baseline / np.std(baseline) * 0.1\n",
    "\n",
    "        # Calcium transients (random spikes convolved with exponential decay)\n",
    "        spike_rate = 0.3 + 0.2 * np.random.rand()  # Hz\n",
    "        spikes = np.random.poisson(spike_rate / sampling_rate, n_timepoints)\n",
    "        # Higher spike rate during social epochs for some neurons\n",
    "        if np.random.rand() > 0.4:  # 60% of neurons are socially modulated\n",
    "            social_mask = behavior_labels == 1\n",
    "            spikes[social_mask] = np.random.poisson(\n",
    "                (spike_rate * 1.8) / sampling_rate, social_mask.sum()\n",
    "            )\n",
    "        # Exponential decay kernel (GCaMP6f-like, ~150ms decay)\n",
    "        tau = 0.15 * sampling_rate  # decay constant in samples\n",
    "        kernel = np.exp(-np.arange(int(5 * tau)) / tau)\n",
    "        kernel /= kernel.sum()\n",
    "        transients = np.convolve(spikes.astype(float), kernel, mode=\"full\")[:n_timepoints]\n",
    "\n",
    "        # Add slow oscillation modulation during social epochs\n",
    "        social_modulation = np.zeros(n_timepoints)\n",
    "        social_modulation[behavior_labels == 1] = (\n",
    "            0.05 * np.sin(2 * np.pi * 5 * t[behavior_labels == 1])  # ~5 Hz theta-like\n",
    "        )\n",
    "\n",
    "        calcium_data[n] = baseline + transients * 0.5 + social_modulation\n",
    "\n",
    "    # Add mild photobleaching\n",
    "    bleaching = np.exp(-t / 2000) * 0.03\n",
    "    calcium_data += bleaching[np.newaxis, :]\n",
    "\n",
    "    print(f\"Synthetic data generated:\")\n",
    "    print(f\"  calcium_data shape: {calcium_data.shape} (neurons x time)\")\n",
    "    print(f\"  behavior_labels shape: {behavior_labels.shape}\")\n",
    "    print(f\"  sampling_rate: {sampling_rate} Hz\")\n",
    "    print(f\"  duration: {duration} sec\")\n",
    "    print(f\"  social epochs: {behavior_labels.sum()} samples ({100*behavior_labels.mean():.1f}%)\")\n",
    "    print(f\"  solo epochs:   {(1-behavior_labels).sum()} samples ({100*(1-behavior_labels.mean()):.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(f\"Using real data:\")\n",
    "    print(f\"  calcium_data shape: {calcium_data.shape}\")\n",
    "    print(f\"  sampling_rate: {sampling_rate} Hz\")\n",
    "    n_timepoints = calcium_data.shape[-1]  # Assumes neurons x time\n",
    "    t = np.arange(n_timepoints) / sampling_rate\n",
    "    duration = n_timepoints / sampling_rate\n",
    "    print(f\"  duration: {duration:.1f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot raw calcium traces for a subset of neurons ---\n",
    "n_show = min(8, calcium_data.shape[0])\n",
    "\n",
    "fig, axes = plt.subplots(n_show, 1, figsize=(16, 2.2 * n_show), sharex=True)\n",
    "if n_show == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(t, calcium_data[i], linewidth=0.5, color=\"#2c3e50\")\n",
    "    ax.set_ylabel(f\"Neuron {i}\", fontsize=9)\n",
    "    ax.tick_params(labelsize=8)\n",
    "\n",
    "    # Shade social epochs\n",
    "    if behavior_labels is not None:\n",
    "        social_mask = behavior_labels == 1\n",
    "        starts = np.where(np.diff(social_mask.astype(int)) == 1)[0]\n",
    "        ends = np.where(np.diff(social_mask.astype(int)) == -1)[0]\n",
    "        if social_mask[0]:\n",
    "            starts = np.concatenate([[0], starts])\n",
    "        if social_mask[-1]:\n",
    "            ends = np.concatenate([ends, [len(social_mask) - 1]])\n",
    "        for s, e in zip(starts, ends):\n",
    "            ax.axvspan(t[s], t[e], alpha=0.15, color=\"#e74c3c\", label=\"Social\" if (i == 0 and s == starts[0]) else None)\n",
    "\n",
    "axes[-1].set_xlabel(\"Time (s)\")\n",
    "axes[0].set_title(\"Raw Calcium Traces (red shading = social epochs)\", fontsize=12)\n",
    "if behavior_labels is not None:\n",
    "    axes[0].legend(loc=\"upper right\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic statistics ---\n",
    "print(\"Calcium signal statistics (across all neurons):\\n\")\n",
    "print(f\"  Mean ΔF/F:     {calcium_data.mean():.4f}\")\n",
    "print(f\"  Std  ΔF/F:     {calcium_data.std():.4f}\")\n",
    "print(f\"  Min  ΔF/F:     {calcium_data.min():.4f}\")\n",
    "print(f\"  Max  ΔF/F:     {calcium_data.max():.4f}\")\n",
    "print(f\"  Median ΔF/F:   {np.median(calcium_data):.4f}\")\n",
    "print(f\"\\n  Nyquist freq:  {sampling_rate / 2:.1f} Hz\")\n",
    "print(f\"  Max resolvable: theta ({sampling_rate/2:.0f} Hz Nyquist allows up to ~{sampling_rate/2 - 1:.0f} Hz)\")\n",
    "\n",
    "# Per-neuron activity levels\n",
    "neuron_stds = calcium_data.std(axis=1)\n",
    "print(f\"\\nPer-neuron std range: [{neuron_stds.min():.4f}, {neuron_stds.max():.4f}]\")\n",
    "print(f\"Potentially inactive neurons (std < 0.01): {(neuron_stds < 0.01).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Distribution of calcium signal values ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Histogram of all values\n",
    "axes[0].hist(calcium_data.ravel(), bins=100, color=\"#3498db\", alpha=0.7, edgecolor=\"white\")\n",
    "axes[0].set_xlabel(\"ΔF/F\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Distribution of all calcium values\")\n",
    "\n",
    "# Per-neuron standard deviations\n",
    "axes[1].bar(range(len(neuron_stds)), np.sort(neuron_stds)[::-1], color=\"#2ecc71\")\n",
    "axes[1].set_xlabel(\"Neuron (sorted)\")\n",
    "axes[1].set_ylabel(\"Std of ΔF/F\")\n",
    "axes[1].set_title(\"Activity level per neuron\")\n",
    "\n",
    "# Correlation matrix (subset)\n",
    "n_corr = min(20, calcium_data.shape[0])\n",
    "corr = np.corrcoef(calcium_data[:n_corr])\n",
    "im = axes[2].imshow(corr, cmap=\"RdBu_r\", vmin=-1, vmax=1, aspect=\"auto\")\n",
    "axes[2].set_xlabel(\"Neuron\")\n",
    "axes[2].set_ylabel(\"Neuron\")\n",
    "axes[2].set_title(f\"Pairwise correlation (first {n_corr} neurons)\")\n",
    "plt.colorbar(im, ax=axes[2], shrink=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Check for photobleaching\n",
    "\n",
    "Photobleaching causes an exponential decay in fluorescence baseline over the recording. This injects power into the lowest frequency bins and **must be corrected** before spectral analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for photobleaching by looking at the mean trace over time\n",
    "mean_trace = calcium_data.mean(axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "# Mean trace\n",
    "axes[0].plot(t, mean_trace, linewidth=0.8, color=\"#2c3e50\")\n",
    "# Linear fit to detect trend\n",
    "slope, intercept = np.polyfit(t, mean_trace, 1)\n",
    "axes[0].plot(t, slope * t + intercept, \"--r\", linewidth=1.5, label=f\"Linear fit (slope={slope:.2e})\")\n",
    "axes[0].set_xlabel(\"Time (s)\")\n",
    "axes[0].set_ylabel(\"Mean ΔF/F\")\n",
    "axes[0].set_title(\"Population mean trace — check for photobleaching\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Rolling mean to see slow trends\n",
    "window_sec = 30  # 30-second rolling window\n",
    "window_samples = int(window_sec * sampling_rate)\n",
    "rolling_mean = pd.Series(mean_trace).rolling(window=window_samples, center=True).mean()\n",
    "axes[1].plot(t, rolling_mean, linewidth=1.2, color=\"#e74c3c\")\n",
    "axes[1].set_xlabel(\"Time (s)\")\n",
    "axes[1].set_ylabel(\"30-sec rolling mean ΔF/F\")\n",
    "axes[1].set_title(\"Slow baseline drift\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if abs(slope) > 1e-5:\n",
    "    print(f\"Baseline drift detected (slope = {slope:.2e} ΔF/F per second).\")\n",
    "    print(\"Recommendation: Detrend before spectral analysis.\")\n",
    "else:\n",
    "    print(\"No significant baseline drift detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Behavioral Epoch Structure\n",
    "\n",
    "Examine the distribution and structure of social vs. solo behavioral labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if behavior_labels is not None:\n",
    "    # Epoch statistics\n",
    "    social_frac = behavior_labels.mean()\n",
    "    solo_frac = 1 - social_frac\n",
    "\n",
    "    # Find epoch boundaries\n",
    "    changes = np.diff(behavior_labels)\n",
    "    epoch_boundaries = np.where(changes != 0)[0] + 1\n",
    "    epoch_starts = np.concatenate([[0], epoch_boundaries])\n",
    "    epoch_ends = np.concatenate([epoch_boundaries, [len(behavior_labels)]])\n",
    "    epoch_labels = [behavior_labels[s] for s in epoch_starts]\n",
    "    epoch_durations = (epoch_ends - epoch_starts) / sampling_rate  # in seconds\n",
    "\n",
    "    social_durations = epoch_durations[np.array(epoch_labels) == 1]\n",
    "    solo_durations = epoch_durations[np.array(epoch_labels) == 0]\n",
    "\n",
    "    print(f\"Behavioral epoch summary:\")\n",
    "    print(f\"  Total epochs: {len(epoch_starts)}\")\n",
    "    print(f\"  Social: {len(social_durations)} epochs, {social_frac*100:.1f}% of total time\")\n",
    "    print(f\"  Solo:   {len(solo_durations)} epochs, {solo_frac*100:.1f}% of total time\")\n",
    "    print(f\"  Class ratio (solo:social): {solo_frac/social_frac:.2f}:1\")\n",
    "    print(f\"\\n  Social epoch duration: mean={social_durations.mean():.1f}s, \"\n",
    "          f\"std={social_durations.std():.1f}s, range=[{social_durations.min():.1f}, {social_durations.max():.1f}]s\")\n",
    "    print(f\"  Solo epoch duration:   mean={solo_durations.mean():.1f}s, \"\n",
    "          f\"std={solo_durations.std():.1f}s, range=[{solo_durations.min():.1f}, {solo_durations.max():.1f}]s\")\n",
    "\n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "    # Class balance pie chart\n",
    "    axes[0].pie([social_frac, solo_frac], labels=[\"Social\", \"Solo\"],\n",
    "                colors=[\"#e74c3c\", \"#3498db\"], autopct=\"%1.1f%%\", startangle=90)\n",
    "    axes[0].set_title(\"Class balance\")\n",
    "\n",
    "    # Epoch duration distributions\n",
    "    bins = np.linspace(0, max(epoch_durations.max(), 60), 20)\n",
    "    axes[1].hist(social_durations, bins=bins, alpha=0.7, color=\"#e74c3c\", label=\"Social\")\n",
    "    axes[1].hist(solo_durations, bins=bins, alpha=0.7, color=\"#3498db\", label=\"Solo\")\n",
    "    axes[1].set_xlabel(\"Epoch duration (s)\")\n",
    "    axes[1].set_ylabel(\"Count\")\n",
    "    axes[1].set_title(\"Epoch duration distribution\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Timeline\n",
    "    axes[2].fill_between(t, behavior_labels, step=\"pre\", alpha=0.5, color=\"#e74c3c\", label=\"Social\")\n",
    "    axes[2].set_xlabel(\"Time (s)\")\n",
    "    axes[2].set_ylabel(\"Label\")\n",
    "    axes[2].set_title(\"Behavioral state timeline\")\n",
    "    axes[2].set_yticks([0, 1])\n",
    "    axes[2].set_yticklabels([\"Solo\", \"Social\"])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No behavioral labels loaded. Spectral analysis will proceed without epoch comparison.\")\n",
    "    print(\"To add labels, load them in Section 3.2 above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Preliminary Spectral Analysis\n",
    "\n",
    "This is the **core signal processing** section. We compute:\n",
    "\n",
    "1. **Power Spectral Density (PSD)** via Welch's method — the primary tool\n",
    "2. **Spectrogram** via STFT — time-frequency visualization\n",
    "3. **Wavelet scalogram** via Morlet CWT — adaptive time-frequency resolution\n",
    "\n",
    "### Key parameters\n",
    "- **Sampling rate**: determines Nyquist frequency (max observable freq = fs/2)\n",
    "- **Window length**: determines frequency resolution (Δf = fs / nperseg)\n",
    "- **Frequency bands of interest**:\n",
    "  - Infraslow: 0.01 – 0.1 Hz\n",
    "  - Slow: 0.1 – 1 Hz\n",
    "  - Delta: 1 – 4 Hz\n",
    "  - Theta: 4 – 7 Hz (upper limit depends on Nyquist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Frequency bands for calcium imaging ---\n",
    "FREQ_BANDS = {\n",
    "    \"infraslow\": (0.01, 0.1),\n",
    "    \"slow\":      (0.1, 1.0),\n",
    "    \"delta\":     (1.0, 4.0),\n",
    "    \"theta\":     (4.0, min(7.0, sampling_rate / 2 - 0.5)),\n",
    "}\n",
    "\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"Nyquist frequency: {sampling_rate / 2} Hz\")\n",
    "print(f\"\\nFrequency bands:\")\n",
    "for name, (lo, hi) in FREQ_BANDS.items():\n",
    "    print(f\"  {name:12s}: {lo:.2f} – {hi:.2f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Power Spectral Density (Welch's method)\n",
    "\n",
    "Welch's method averages periodograms over overlapping windows to reduce variance. This is the standard approach for calcium imaging spectral analysis (Bhatt et al. 2013, Frontiers in Neural Circuits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welch PSD parameters\n",
    "nperseg = int(10 * sampling_rate)   # 10-second windows\n",
    "noverlap = nperseg // 2             # 50% overlap\n",
    "nfft = max(1024, nperseg * 2)       # Zero-pad for smoother PSD\n",
    "\n",
    "print(f\"Welch parameters:\")\n",
    "print(f\"  Window length: {nperseg} samples ({nperseg/sampling_rate:.1f} s)\")\n",
    "print(f\"  Overlap: {noverlap} samples ({noverlap/sampling_rate:.1f} s)\")\n",
    "print(f\"  FFT points: {nfft}\")\n",
    "print(f\"  Frequency resolution: {sampling_rate / nperseg:.3f} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PSD for all neurons\n",
    "n_neurons = calcium_data.shape[0]\n",
    "f_welch, psd_all = welch(\n",
    "    calcium_data,\n",
    "    fs=sampling_rate,\n",
    "    nperseg=nperseg,\n",
    "    noverlap=noverlap,\n",
    "    nfft=nfft,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"PSD computed: {psd_all.shape} (neurons x frequency bins)\")\n",
    "print(f\"Frequency range: {f_welch[1]:.4f} – {f_welch[-1]:.2f} Hz ({len(f_welch)} bins)\")\n",
    "\n",
    "# Plot: Mean PSD across all neurons\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Linear scale\n",
    "mean_psd = psd_all.mean(axis=0)\n",
    "sem_psd = psd_all.std(axis=0) / np.sqrt(n_neurons)\n",
    "\n",
    "axes[0].plot(f_welch, mean_psd, color=\"#2c3e50\", linewidth=1.5)\n",
    "axes[0].fill_between(f_welch, mean_psd - sem_psd, mean_psd + sem_psd, alpha=0.3, color=\"#3498db\")\n",
    "axes[0].set_xlabel(\"Frequency (Hz)\")\n",
    "axes[0].set_ylabel(\"Power spectral density\")\n",
    "axes[0].set_title(\"Mean PSD (linear scale)\")\n",
    "\n",
    "# Shade frequency bands\n",
    "band_colors = {\"infraslow\": \"#f39c12\", \"slow\": \"#2ecc71\", \"delta\": \"#3498db\", \"theta\": \"#9b59b6\"}\n",
    "for name, (lo, hi) in FREQ_BANDS.items():\n",
    "    for ax in axes:\n",
    "        ax.axvspan(lo, hi, alpha=0.1, color=band_colors[name], label=name)\n",
    "\n",
    "# Log-log scale (reveals 1/f structure)\n",
    "axes[1].loglog(f_welch[1:], mean_psd[1:], color=\"#2c3e50\", linewidth=1.5)\n",
    "axes[1].fill_between(f_welch[1:], (mean_psd - sem_psd)[1:], (mean_psd + sem_psd)[1:],\n",
    "                      alpha=0.3, color=\"#3498db\")\n",
    "axes[1].set_xlabel(\"Frequency (Hz)\")\n",
    "axes[1].set_ylabel(\"PSD\")\n",
    "axes[1].set_title(\"Mean PSD (log-log scale — check for 1/f)\")\n",
    "\n",
    "axes[0].legend(fontsize=8, loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PSD heatmap across all neurons ---\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Limit to meaningful frequency range\n",
    "freq_mask = f_welch <= 10  # Up to 10 Hz (or Nyquist)\n",
    "psd_plot = 10 * np.log10(psd_all[:, freq_mask] + 1e-12)  # dB scale\n",
    "\n",
    "im = ax.imshow(psd_plot, aspect=\"auto\", cmap=\"viridis\",\n",
    "               extent=[f_welch[freq_mask][0], f_welch[freq_mask][-1], n_neurons, 0])\n",
    "ax.set_xlabel(\"Frequency (Hz)\")\n",
    "ax.set_ylabel(\"Neuron\")\n",
    "ax.set_title(\"PSD heatmap across neurons (dB)\")\n",
    "plt.colorbar(im, label=\"Power (dB)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Spectrogram (STFT)\n",
    "\n",
    "The Short-Time Fourier Transform shows how spectral content **evolves over time**. This lets us visually check whether spectral features change when behavior switches between social and solo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectrogram for a representative neuron\n",
    "# Pick the neuron with highest variance (most active)\n",
    "active_neuron_idx = np.argmax(calcium_data.std(axis=1))\n",
    "print(f\"Most active neuron: #{active_neuron_idx} (std = {calcium_data[active_neuron_idx].std():.4f})\")\n",
    "\n",
    "# STFT parameters\n",
    "stft_nperseg = int(5 * sampling_rate)  # 5-second window\n",
    "f_stft, t_stft, Zxx = stft(\n",
    "    calcium_data[active_neuron_idx],\n",
    "    fs=sampling_rate,\n",
    "    nperseg=stft_nperseg,\n",
    "    noverlap=stft_nperseg // 2\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 10), gridspec_kw={\"height_ratios\": [1, 3, 1]})\n",
    "\n",
    "# Top: raw trace\n",
    "axes[0].plot(t, calcium_data[active_neuron_idx], linewidth=0.5, color=\"#2c3e50\")\n",
    "if behavior_labels is not None:\n",
    "    social_mask = behavior_labels == 1\n",
    "    starts = np.where(np.diff(social_mask.astype(int)) == 1)[0]\n",
    "    ends = np.where(np.diff(social_mask.astype(int)) == -1)[0]\n",
    "    if social_mask[0]: starts = np.concatenate([[0], starts])\n",
    "    if social_mask[-1]: ends = np.concatenate([ends, [len(social_mask) - 1]])\n",
    "    for s, e in zip(starts, ends):\n",
    "        axes[0].axvspan(t[s], t[e], alpha=0.2, color=\"#e74c3c\")\n",
    "axes[0].set_ylabel(\"ΔF/F\")\n",
    "axes[0].set_title(f\"Neuron #{active_neuron_idx} — Calcium trace + Spectrogram\")\n",
    "\n",
    "# Middle: spectrogram\n",
    "freq_limit = min(10, sampling_rate / 2)\n",
    "freq_mask_stft = f_stft <= freq_limit\n",
    "axes[1].pcolormesh(t_stft, f_stft[freq_mask_stft],\n",
    "                   10 * np.log10(np.abs(Zxx[freq_mask_stft]) ** 2 + 1e-12),\n",
    "                   shading=\"gouraud\", cmap=\"magma\")\n",
    "axes[1].set_ylabel(\"Frequency (Hz)\")\n",
    "axes[1].set_title(\"Spectrogram (STFT, dB)\")\n",
    "\n",
    "# Overlay band boundaries\n",
    "for name, (lo, hi) in FREQ_BANDS.items():\n",
    "    if hi <= freq_limit:\n",
    "        axes[1].axhline(lo, color=\"white\", linewidth=0.5, alpha=0.5, linestyle=\"--\")\n",
    "        axes[1].axhline(hi, color=\"white\", linewidth=0.5, alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "# Bottom: behavioral labels\n",
    "if behavior_labels is not None:\n",
    "    axes[2].fill_between(t, behavior_labels, step=\"pre\", alpha=0.7, color=\"#e74c3c\")\n",
    "    axes[2].set_ylabel(\"Social\")\n",
    "    axes[2].set_yticks([0, 1])\n",
    "    axes[2].set_yticklabels([\"Solo\", \"Social\"])\n",
    "axes[2].set_xlabel(\"Time (s)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Wavelet Scalogram (Morlet CWT)\n",
    "\n",
    "Morlet wavelets provide **adaptive time-frequency resolution**: better frequency resolution at low frequencies (where calcium signals live) and better time resolution at higher frequencies. This makes them particularly well-suited for non-stationary neural signals (Cohen 2019, NeuroImage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morlet wavelet transform on the active neuron\n",
    "# Define scales corresponding to our frequency bands of interest\n",
    "freq_range = np.linspace(0.5, min(9, sampling_rate / 2 - 0.5), 60)\n",
    "wavelet_name = \"cmor1.5-1.0\"  # Complex Morlet wavelet (bandwidth=1.5, center_freq=1.0)\n",
    "\n",
    "# Compute scales from desired frequencies\n",
    "scales = pywt.central_frequency(wavelet_name) * sampling_rate / freq_range\n",
    "\n",
    "# Use a shorter segment for computation (wavelets are expensive)\n",
    "segment_duration = min(120, duration)  # First 120 seconds\n",
    "segment_samples = int(segment_duration * sampling_rate)\n",
    "sig_segment = calcium_data[active_neuron_idx, :segment_samples]\n",
    "t_segment = t[:segment_samples]\n",
    "\n",
    "print(f\"Computing CWT on {segment_duration}s segment ({segment_samples} samples)...\")\n",
    "coeffs, freqs = pywt.cwt(sig_segment, scales, wavelet_name, sampling_period=1/sampling_rate)\n",
    "power = np.abs(coeffs) ** 2\n",
    "print(f\"Scalogram shape: {power.shape} (frequencies x time)\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 8), gridspec_kw={\"height_ratios\": [1, 3]})\n",
    "\n",
    "# Top: calcium trace\n",
    "axes[0].plot(t_segment, sig_segment, linewidth=0.5, color=\"#2c3e50\")\n",
    "if behavior_labels is not None:\n",
    "    bl_seg = behavior_labels[:segment_samples]\n",
    "    social_mask = bl_seg == 1\n",
    "    starts = np.where(np.diff(social_mask.astype(int)) == 1)[0]\n",
    "    ends = np.where(np.diff(social_mask.astype(int)) == -1)[0]\n",
    "    if social_mask[0]: starts = np.concatenate([[0], starts])\n",
    "    if social_mask[-1]: ends = np.concatenate([ends, [len(social_mask) - 1]])\n",
    "    for s, e in zip(starts, ends):\n",
    "        axes[0].axvspan(t_segment[s], t_segment[e], alpha=0.2, color=\"#e74c3c\")\n",
    "axes[0].set_ylabel(\"ΔF/F\")\n",
    "axes[0].set_title(f\"Neuron #{active_neuron_idx} — Morlet Wavelet Scalogram\")\n",
    "\n",
    "# Bottom: scalogram\n",
    "axes[1].pcolormesh(t_segment, freqs, 10 * np.log10(power + 1e-12),\n",
    "                   shading=\"gouraud\", cmap=\"magma\")\n",
    "axes[1].set_ylabel(\"Frequency (Hz)\")\n",
    "axes[1].set_xlabel(\"Time (s)\")\n",
    "axes[1].set_title(\"Wavelet power (dB)\")\n",
    "\n",
    "# Mark frequency bands\n",
    "for name, (lo, hi) in FREQ_BANDS.items():\n",
    "    axes[1].axhline(lo, color=\"white\", linewidth=0.5, alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Social vs. Solo: Initial Spectral Comparison\n",
    "\n",
    "The key test: do spectral features **differ** between social and solo epochs? This is the foundation of our classification approach.\n",
    "\n",
    "### Method\n",
    "1. Segment calcium traces into non-overlapping windows within each behavioral state\n",
    "2. Compute spectral features per window\n",
    "3. Compare distributions between social and solo conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectral_features(segment, fs, freq_bands):\n",
    "    \"\"\"Compute spectral features from a 1D signal segment.\n",
    "\n",
    "    Returns a dict of features: band powers, spectral entropy,\n",
    "    peak frequency, spectral centroid, and band power ratios.\n",
    "    \"\"\"\n",
    "    # Welch PSD\n",
    "    nperseg_local = min(len(segment), int(5 * fs))\n",
    "    f, psd = welch(segment, fs=fs, nperseg=nperseg_local, noverlap=nperseg_local // 2)\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # Band powers\n",
    "    for name, (lo, hi) in freq_bands.items():\n",
    "        mask = (f >= lo) & (f < hi)\n",
    "        if mask.sum() > 0:\n",
    "            features[f\"power_{name}\"] = np.trapz(psd[mask], f[mask])\n",
    "        else:\n",
    "            features[f\"power_{name}\"] = 0.0\n",
    "\n",
    "    # Total power\n",
    "    total_power = np.trapz(psd[f > 0], f[f > 0])\n",
    "    features[\"total_power\"] = total_power\n",
    "\n",
    "    # Relative band powers\n",
    "    if total_power > 0:\n",
    "        for name in freq_bands:\n",
    "            features[f\"relpower_{name}\"] = features[f\"power_{name}\"] / total_power\n",
    "\n",
    "    # Spectral entropy (Shannon entropy of normalized PSD)\n",
    "    psd_norm = psd[f > 0] / (psd[f > 0].sum() + 1e-12)\n",
    "    psd_norm = psd_norm[psd_norm > 0]\n",
    "    features[\"spectral_entropy\"] = -np.sum(psd_norm * np.log2(psd_norm))\n",
    "\n",
    "    # Peak frequency\n",
    "    features[\"peak_freq\"] = f[np.argmax(psd)]\n",
    "\n",
    "    # Spectral centroid\n",
    "    if psd[f > 0].sum() > 0:\n",
    "        features[\"spectral_centroid\"] = np.sum(f[f > 0] * psd[f > 0]) / np.sum(psd[f > 0])\n",
    "    else:\n",
    "        features[\"spectral_centroid\"] = 0.0\n",
    "\n",
    "    # Spectral edge frequency (90% of power)\n",
    "    cum_power = np.cumsum(psd[f > 0])\n",
    "    if cum_power[-1] > 0:\n",
    "        edge_idx = np.searchsorted(cum_power, 0.9 * cum_power[-1])\n",
    "        features[\"spectral_edge_90\"] = f[f > 0][min(edge_idx, len(f[f > 0]) - 1)]\n",
    "    else:\n",
    "        features[\"spectral_edge_90\"] = 0.0\n",
    "\n",
    "    # Band power ratios\n",
    "    if features.get(\"power_delta\", 0) > 0:\n",
    "        features[\"theta_delta_ratio\"] = features.get(\"power_theta\", 0) / features[\"power_delta\"]\n",
    "    else:\n",
    "        features[\"theta_delta_ratio\"] = 0.0\n",
    "\n",
    "    return features\n",
    "\n",
    "print(\"Feature extraction function defined.\")\n",
    "print(f\"Features per segment: {len(compute_spectral_features(calcium_data[0, :int(10*sampling_rate)], sampling_rate, FREQ_BANDS))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if behavior_labels is not None:\n",
    "    # Segment data into fixed-length windows within each behavioral state\n",
    "    window_sec = 5.0  # 5-second windows\n",
    "    window_samples = int(window_sec * sampling_rate)\n",
    "    min_epoch_samples = window_samples  # Skip epochs shorter than one window\n",
    "\n",
    "    all_features = []  # List of (features_dict, label, neuron_idx)\n",
    "\n",
    "    # Process each neuron\n",
    "    for neuron_idx in range(n_neurons):\n",
    "        trace = calcium_data[neuron_idx]\n",
    "\n",
    "        # Walk through the signal in windows\n",
    "        for start in range(0, len(trace) - window_samples, window_samples):\n",
    "            end = start + window_samples\n",
    "            window_labels = behavior_labels[start:end]\n",
    "\n",
    "            # Only use windows that are purely one state (>90% same label)\n",
    "            label_frac = window_labels.mean()\n",
    "            if label_frac > 0.9:\n",
    "                label = 1  # Social\n",
    "            elif label_frac < 0.1:\n",
    "                label = 0  # Solo\n",
    "            else:\n",
    "                continue  # Skip mixed windows\n",
    "\n",
    "            feats = compute_spectral_features(trace[start:end], sampling_rate, FREQ_BANDS)\n",
    "            feats[\"neuron\"] = neuron_idx\n",
    "            feats[\"label\"] = label\n",
    "            feats[\"start_time\"] = start / sampling_rate\n",
    "            all_features.append(feats)\n",
    "\n",
    "    df_features = pd.DataFrame(all_features)\n",
    "    print(f\"Feature extraction complete:\")\n",
    "    print(f\"  Total windows: {len(df_features)}\")\n",
    "    print(f\"  Social windows: {(df_features['label'] == 1).sum()}\")\n",
    "    print(f\"  Solo windows: {(df_features['label'] == 0).sum()}\")\n",
    "    print(f\"  Features per window: {len([c for c in df_features.columns if c not in ['neuron', 'label', 'start_time']])}\")\n",
    "    print(f\"\\n{df_features.head()}\")\n",
    "else:\n",
    "    print(\"No behavioral labels — cannot compare conditions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if behavior_labels is not None:\n",
    "    # --- Compare key spectral features between social and solo ---\n",
    "    feature_cols = [c for c in df_features.columns\n",
    "                    if c not in [\"neuron\", \"label\", \"start_time\"]]\n",
    "\n",
    "    # Select the most interpretable features for visualization\n",
    "    plot_features = [\n",
    "        \"power_delta\", \"power_theta\", \"spectral_entropy\",\n",
    "        \"spectral_centroid\", \"theta_delta_ratio\", \"total_power\"\n",
    "    ]\n",
    "    plot_features = [f for f in plot_features if f in df_features.columns]\n",
    "\n",
    "    n_plot = len(plot_features)\n",
    "    fig, axes = plt.subplots(2, (n_plot + 1) // 2, figsize=(16, 8))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, feat in enumerate(plot_features):\n",
    "        social_vals = df_features.loc[df_features[\"label\"] == 1, feat]\n",
    "        solo_vals = df_features.loc[df_features[\"label\"] == 0, feat]\n",
    "\n",
    "        axes[i].hist(solo_vals, bins=30, alpha=0.6, color=\"#3498db\", label=\"Solo\", density=True)\n",
    "        axes[i].hist(social_vals, bins=30, alpha=0.6, color=\"#e74c3c\", label=\"Social\", density=True)\n",
    "        axes[i].set_title(feat, fontsize=10)\n",
    "        axes[i].legend(fontsize=8)\n",
    "\n",
    "        # Quick statistical test (Mann-Whitney U)\n",
    "        from scipy.stats import mannwhitneyu\n",
    "        stat, pval = mannwhitneyu(social_vals, solo_vals, alternative=\"two-sided\")\n",
    "        sig = \"***\" if pval < 0.001 else \"**\" if pval < 0.01 else \"*\" if pval < 0.05 else \"ns\"\n",
    "        axes[i].set_xlabel(f\"p={pval:.2e} {sig}\", fontsize=9)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(n_plot, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.suptitle(\"Spectral Features: Social vs. Solo\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if behavior_labels is not None:\n",
    "    # --- Average PSD: social vs solo ---\n",
    "    # Compute PSD separately for social and solo segments\n",
    "    social_psds = []\n",
    "    solo_psds = []\n",
    "\n",
    "    for neuron_idx in range(n_neurons):\n",
    "        trace = calcium_data[neuron_idx]\n",
    "        for start in range(0, len(trace) - window_samples, window_samples):\n",
    "            end = start + window_samples\n",
    "            label_frac = behavior_labels[start:end].mean()\n",
    "\n",
    "            nperseg_local = min(window_samples, int(5 * sampling_rate))\n",
    "            f_seg, psd_seg = welch(trace[start:end], fs=sampling_rate,\n",
    "                                   nperseg=nperseg_local, noverlap=nperseg_local // 2)\n",
    "\n",
    "            if label_frac > 0.9:\n",
    "                social_psds.append(psd_seg)\n",
    "            elif label_frac < 0.1:\n",
    "                solo_psds.append(psd_seg)\n",
    "\n",
    "    social_psds = np.array(social_psds)\n",
    "    solo_psds = np.array(solo_psds)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    # Mean PSD comparison\n",
    "    social_mean = social_psds.mean(axis=0)\n",
    "    social_sem = social_psds.std(axis=0) / np.sqrt(len(social_psds))\n",
    "    solo_mean = solo_psds.mean(axis=0)\n",
    "    solo_sem = solo_psds.std(axis=0) / np.sqrt(len(solo_psds))\n",
    "\n",
    "    axes[0].semilogy(f_seg, social_mean, color=\"#e74c3c\", linewidth=1.5, label=f\"Social (n={len(social_psds)})\")\n",
    "    axes[0].fill_between(f_seg, social_mean - social_sem, social_mean + social_sem, alpha=0.3, color=\"#e74c3c\")\n",
    "    axes[0].semilogy(f_seg, solo_mean, color=\"#3498db\", linewidth=1.5, label=f\"Solo (n={len(solo_psds)})\")\n",
    "    axes[0].fill_between(f_seg, solo_mean - solo_sem, solo_mean + solo_sem, alpha=0.3, color=\"#3498db\")\n",
    "    axes[0].set_xlabel(\"Frequency (Hz)\")\n",
    "    axes[0].set_ylabel(\"PSD (log scale)\")\n",
    "    axes[0].set_title(\"Mean PSD: Social vs. Solo\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Shade bands\n",
    "    for name, (lo, hi) in FREQ_BANDS.items():\n",
    "        axes[0].axvspan(lo, hi, alpha=0.08, color=band_colors[name])\n",
    "\n",
    "    # Log ratio (social / solo)\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        log_ratio = np.log2(social_mean / (solo_mean + 1e-12))\n",
    "    axes[1].plot(f_seg, log_ratio, color=\"#2c3e50\", linewidth=1.5)\n",
    "    axes[1].axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n",
    "    axes[1].set_xlabel(\"Frequency (Hz)\")\n",
    "    axes[1].set_ylabel(\"log2(Social / Solo)\")\n",
    "    axes[1].set_title(\"PSD log-ratio (>0 = more power during social)\")\n",
    "    for name, (lo, hi) in FREQ_BANDS.items():\n",
    "        axes[1].axvspan(lo, hi, alpha=0.08, color=band_colors[name])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Social PSD windows: {len(social_psds)}\")\n",
    "    print(f\"Solo PSD windows: {len(solo_psds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Quick classification sanity check\n",
    "\n",
    "Fit a simple classifier to see if spectral features carry **any** discriminative information. This is NOT the final analysis — just a sanity check.\n",
    "\n",
    "**Important:** We use block cross-validation to avoid data leakage from temporal autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if behavior_labels is not None:\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import GroupKFold\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "    # Prepare feature matrix\n",
    "    feature_cols_clean = [c for c in df_features.columns\n",
    "                          if c not in [\"neuron\", \"label\", \"start_time\"]]\n",
    "    X = df_features[feature_cols_clean].values\n",
    "    y = df_features[\"label\"].values\n",
    "\n",
    "    # Groups for block CV: use time bins to prevent temporal leakage\n",
    "    # Each group is a ~30-second block of time\n",
    "    groups = (df_features[\"start_time\"] // 30).astype(int).values\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Replace NaN/Inf with 0 (edge cases from very short segments)\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # Block cross-validation\n",
    "    n_unique_groups = len(np.unique(groups))\n",
    "    n_splits = min(5, n_unique_groups)\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    classifiers = {\n",
    "        \"LDA\": LinearDiscriminantAnalysis(),\n",
    "        \"SVM (linear)\": SVC(kernel=\"linear\", probability=True),\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    }\n",
    "\n",
    "    print(f\"Classification sanity check ({n_splits}-fold GroupKFold CV)\")\n",
    "    print(f\"  Samples: {len(y)} ({(y==1).sum()} social, {(y==0).sum()} solo)\")\n",
    "    print(f\"  Features: {X_scaled.shape[1]}\")\n",
    "    print(f\"  Groups: {n_unique_groups} time blocks\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        fold_accs = []\n",
    "        fold_aucs = []\n",
    "\n",
    "        for train_idx, test_idx in gkf.split(X_scaled, y, groups):\n",
    "            X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                continue\n",
    "\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            fold_accs.append(accuracy_score(y_test, y_pred))\n",
    "            fold_aucs.append(roc_auc_score(y_test, y_proba))\n",
    "\n",
    "        if fold_accs:\n",
    "            print(f\"\\n  {clf_name}:\")\n",
    "            print(f\"    Accuracy: {np.mean(fold_accs):.3f} +/- {np.std(fold_accs):.3f}\")\n",
    "            print(f\"    AUC:      {np.mean(fold_aucs):.3f} +/- {np.std(fold_aucs):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if behavior_labels is not None:\n",
    "    # --- Permutation test for chance-level baseline ---\n",
    "    n_permutations = 200\n",
    "    perm_accs = []\n",
    "\n",
    "    print(f\"Running permutation test ({n_permutations} shuffles)...\")\n",
    "    clf_perm = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    for perm_i in range(n_permutations):\n",
    "        y_shuffled = np.random.permutation(y)\n",
    "        fold_accs_perm = []\n",
    "        for train_idx, test_idx in gkf.split(X_scaled, y_shuffled, groups):\n",
    "            X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "            y_train, y_test = y_shuffled[train_idx], y_shuffled[test_idx]\n",
    "            if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                continue\n",
    "            clf_perm.fit(X_train, y_train)\n",
    "            fold_accs_perm.append(accuracy_score(y_test, clf_perm.predict(X_test)))\n",
    "        if fold_accs_perm:\n",
    "            perm_accs.append(np.mean(fold_accs_perm))\n",
    "\n",
    "    perm_accs = np.array(perm_accs)\n",
    "    real_acc = np.mean(fold_accs)  # From last classifier above\n",
    "    p_value = np.mean(perm_accs >= real_acc)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.hist(perm_accs, bins=30, color=\"#95a5a6\", alpha=0.7, label=\"Permutation null\")\n",
    "    ax.axvline(real_acc, color=\"#e74c3c\", linewidth=2, linestyle=\"--\",\n",
    "               label=f\"Real accuracy = {real_acc:.3f}\")\n",
    "    ax.set_xlabel(\"Accuracy\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"Permutation test (p = {p_value:.4f})\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nPermutation test result:\")\n",
    "    print(f\"  Real accuracy: {real_acc:.3f}\")\n",
    "    print(f\"  Chance mean:   {perm_accs.mean():.3f} +/- {perm_accs.std():.3f}\")\n",
    "    print(f\"  p-value:       {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"  -> Spectral features carry SIGNIFICANT discriminative information!\")\n",
    "    else:\n",
    "        print(\"  -> Not significant at p<0.05. May need more data or better features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Summary & Next Steps\n",
    "\n",
    "### What we learned from this EDA\n",
    "\n",
    "| Question | Finding |\n",
    "|----------|---------|\n",
    "| Data format | *(fill in after running)* |\n",
    "| Sampling rate | *(fill in)* Hz — Nyquist = *(fill in)* Hz |\n",
    "| Photobleaching | Present / Not present — needs detrending? |\n",
    "| Class balance | Social: *X*% vs Solo: *Y*% — balanced / imbalanced? |\n",
    "| PSD structure | 1/f present? Peaks in specific bands? |\n",
    "| Social vs Solo PSD | Visible differences in which bands? |\n",
    "| Classification | Accuracy: *X*% (chance: *Y*%, p = *Z*) |\n",
    "\n",
    "### Next steps for the full project\n",
    "\n",
    "1. **Load real data** — replace synthetic data with actual calcium traces from EDGE notebooks\n",
    "2. **Preprocessing pipeline** — implement detrending, motion artifact removal, z-scoring\n",
    "3. **Feature engineering** — refine spectral features based on EDA findings\n",
    "4. **Proper cross-validation** — implement leave-one-bout-out or leave-one-animal-out CV\n",
    "5. **Movement confound control** — add movement speed as a covariate\n",
    "6. **Wavelet-based features** — extract time-resolved spectral features from CWT\n",
    "7. **Final report** — write up with publication-quality figures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
